= Web Scraping

Taking information provided by a web page and transforming it into data that
can be searched / stored.

== Warning

WARNING: Scraping is often a violation of the terms of service for a website.
The tools and techniques presented here are for educational purposes only and
should not be used without the permission of the website owner.

== How is it done?

[plantuml, scraping, svg, width=80%]
....
@startuml
rectangle html as "HTML"
rectangle scraper as "Scraper"
rectangle data as "Parsed Data"

html -> scraper: requests / Selenium
scraper -> data: Beautiful Soup / Selenium
@enduml
....

=== Requests / Beautiful Soup

[.shrink]
* Typically the fastest option
* https://requests.readthedocs.io/en/master/[Requests is the most popular Python
  HTTP library]
* https://www.crummy.com/software/BeautifulSoup/bs4/doc/[Beautiful Soup is the
  most popular Python library for parsing HTML]
* https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags[Don't
  parse HTML with regex, you will open a portal to hell]

=== Selenium

[.shrink]
* https://selenium-python.readthedocs.io/[Selenium is a headless browser]
* Python bindings and drivers for:
** Chrome
** Edge
** Firefox
** Safari
* You can make Selenium look _exactly_ like a typical client
* Heavy and slow

=== Tips and Tricks

[.shrink]
* Both options support cookies for session management and login. This is usually
  the hardest part.
* Both allow you to change your User-Agent.
* Throttle your requests or you will be found out. Use the methods the client
  uses, even if they're poorly implemented.
* Use CSS selectors over XML or other methods. They're more useful in other
  contexts (JQuery, etc.).
* Get comfortable with your browser's developer tools.
* Try to uncover as much of the underlying back-end as possible. While you may not
  have access to an API, you may be able to reverse engineer one.

== Example

* I've built and example of both methods running in a container that can be found
  in the `scraper_demo` directory of the class repo.
* They include a container to run them that can be built with
  `docker build -t scraper .`
* Then you can run them with `docker run scraper python requests_bs.py` or
  `docker run --shm-size=2g scraper python selenium_demo.py`

=== Requests / Beautiful Soup

.scraper_demo/requests_bs.py
[source, python]
----
import requests
from bs4 import BeautifulSoup

# requests
query = "lifepo4"
response = requests.get(f"https://cnj.craigslist.org/search/sss?sort=date&query={query}")

# beautiful soup
soup = BeautifulSoup(response.text, 'html.parser')
for result in soup.select('a.result-title'):
    data_id = result['data-id']
    title = result.string
    datetime = result.find_previous_sibling('time')['datetime']
    print(f"{data_id},{datetime},{title}")
----

=== Selenium

.scraper_demo/selenium_demo.py
[source, python]
----
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.common.exceptions import NoSuchElementException

options = Options()
options.headless = True
driver = webdriver.Firefox(options=options)

query = 'lifepo4'
driver.get(f'http://amazon.com/s?k={query}')
for element in driver.find_elements_by_css_selector('div.a-section.a-spacing-medium'):
    try:
        name = element.find_element_by_css_selector('h2 span').text
        price_whole = element.find_element_by_css_selector('span.a-price-whole').text
        price_fraction = element.find_element_by_css_selector('span.a-price-fraction').text
        print(f"\"{name}\",{price_whole}.{price_fraction}")
    except NoSuchElementException:
        pass

driver.close()
----
